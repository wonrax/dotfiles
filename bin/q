#!/usr/bin/env nu
# q - Quick LLM queries with LLM CLIs
# Inspired by https://entropicthoughts.com/q
# Created using Deepseek R1 with supervision.

# Predefined system prompts
const system_prompts = {
    brief: "Answer in as few words as possible. Use a brief style with short replies."
    detailed: "Provide a thorough explanation with examples and technical details."
    commit: "Generate a concise commit message based on the jj diff. Sample the jj log
    for conventional format used in the current project. If there isn't any,
    fallback to conventional commit message. Then use that message to jj
    describe automatically without asking for user confirmation because the
    user can then redescribe it later if needed.

    Rules:
    - **IMPORTANT:** Try to limit the first line message length of 72
    characters unless necessary.
    - Only let the command output to stdout, DO NOT redirect it to any file.
    - For non-trivial changes, provide a multi-line commit message with a
    short summary in the first line, followed by a blank line, and then a
    detailed description of the changes. Avoid overusing bullet points.

    For acquiring the diff, use
    `jj diff --no-pager --config ui.diff.tool='["git", "--no-pager", "diff", "--no-color", "-U32", "$left", "$right"]'`
    to get the machine readable diff.

    For sampling the commit message, use
    `jj log -n20 -r ::@ --color=never -T builtin_log_oneline --no-pager`.

    Run `jj status` first to see which files are changed, then you can proceed
    to get the diff by running the above command with one or multiple files
    specified. This is to avoid getting a huge diff being truncated when there
    are many changes or showing lock files.

    Finally, run `jj describe --message \"<COMMIT MESSAGE>\"` to create the
    commit with the generated message.

    In the end, do not provide any additional text or explanation, since the UI already shows which command you have run.
    "
}

# Get available system prompt names
def get-prompt-names [] {
    $system_prompts | columns
}

# Main function
export def main [
    ...prompt: string  # Prompt as last arguments without quotes
    --recipe (-r): string = "brief"  # Predefined prompt name
    --context (-c): string = ""  # Predefined context string
] {
    # Validate system prompt name
    let valid_prompts = (get-prompt-names)
    if $recipe not-in $valid_prompts {
        error make {
            msg: $"Invalid system prompt '($recipe)'",
            label: {
                text: $"Valid options: ($valid_prompts | str join ', ')"
                span: (metadata $recipe).span
            }
        }
    }

    # Capture piped input if any
    let piped_input = $in

    # Combine all prompt arguments into a single string
    let user_prompt = ($prompt | str join " ")

    # Build the full prompt
    let final_system_prompts = $"
        You are a helpful AI assistant. Your task is to assist the user with
        their queries based on the provided system prompt.

        This chat is oneshot, meaning the user won't be able to ask follow-up
        questions, and you should not ask for any clarifications or additional
        information. Provide a complete answer based on the provided context
        and perform any necessary actions without further interaction.

        ($system_prompts | get $recipe)

        (if $context != "" { $"CONTEXT:\n($context)\n\n" })
        (if not ($piped_input | is-empty) { $"INPUT:\n($piped_input)\n\n" })
    " | str trim

    if $final_system_prompts == "" {
        error make {msg: "Error: No input provided"}
    }

    opencode run --model github-copilot/gemini-3-flash-preview $final_system_prompts $user_prompt
}
