#!/usr/bin/env nu
# q - Quick LLM queries with LLM CLIs
# Inspired by https://entropicthoughts.com/q
# Created using Deepseek R1 with supervision.

# Predefined system prompts
const system_prompts = {
    brief: "Answer in as few words as possible. Use a brief style with short replies."
    detailed: "Provide a thorough explanation with examples and technical details."
    commit: "Task: Update the description of a specific revision (defaulting to
    `@` if unspecified).
    Instructions:

    1. Target Identification:
       - Identify the target revision from the user's request.
       - If no revision is explicitly specified, default to `@` (current
       working copy).
       - Let variable $REV be the identified revision.

    2. Context Gathering (Concurrent Execution):
       - Run `jj show -r $REV --summary` to list modified files.
       - To prevent output truncation, do not run a single monolithic diff.
         Instead, generate individual diff commands for each changed file found
         in the status.
       - Command pattern: `jj diff --git --no-pager -r $REV <filename>`
       - Execute these diff commands concurrently (or in a single batch
         request) to maximize speed and capturing ability.
       - Sample history: `jj log -n20 -r ::$REV --color=never -T builtin_log_oneline --no-pager`.

    3. Message Generation:
       - Analyze the diffs and history.
       - Follow the Conventional Commits format found in the logs.
       - Header: Max 72 characters, imperative mood.
       - Body: Use for non-trivial changes. Separate from header with a blank line.
         Max 72 characters per line.

    4. Action:
       - Execute `jj describe -r $REV --message \"<YOUR_MESSAGE>\"` to apply the change.
       - Output ONLY the result of this command. Do not ask for confirmation.

    Final Output Rule: Do not write any explanations, preambles, or post-scripts."
}

# Get available system prompt names
def get-prompt-names [] {
    $system_prompts | columns
}

# Main function
export def main [
    ...prompt: string  # Prompt as last arguments without quotes
    --recipe (-r): string = "brief"  # Predefined prompt name
    --context (-c): string = ""  # Predefined context string
] {
    # Validate system prompt name
    let valid_prompts = (get-prompt-names)
    if $recipe not-in $valid_prompts {
        error make {
            msg: $"Invalid system prompt '($recipe)'",
            label: {
                text: $"Valid options: ($valid_prompts | str join ', ')"
                span: (metadata $recipe).span
            }
        }
    }

    # Capture piped input if any
    let piped_input = $in

    # Combine all prompt arguments into a single string
    let user_prompt = ($prompt | str join " ")

    # Build the full prompt
    let final_system_prompts = $"
        You are a helpful AI assistant. Your task is to assist the user with
        their queries based on the provided system prompt.

        This chat is oneshot, meaning the user won't be able to ask follow-up
        questions, and you should not ask for any clarifications or additional
        information. Provide a complete answer based on the provided context
        and perform any necessary actions without further interaction.

        ($system_prompts | get $recipe)

        (if $context != "" { $"CONTEXT:\n($context)\n\n" })
        (if not ($piped_input | is-empty) { $"INPUT:\n($piped_input)\n\n" })
    " | str trim

    if $final_system_prompts == "" {
        error make {msg: "Error: No input provided"}
    }

    opencode run --model github-copilot/gemini-3-flash-preview $final_system_prompts $user_prompt
}
